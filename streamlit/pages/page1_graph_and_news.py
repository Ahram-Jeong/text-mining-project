import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import requests as req
from bs4 import BeautifulSoup as bs

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
final_data = pd.read_csv("data/doc_tone_base_rate.csv")
df = pd.DataFrame(final_data)

st.title("ì˜ì‚¬ë¡ ì–´ì¡°ì— ë”°ë¥¸ ê¸ˆë¦¬ ì˜ˆì¸¡")
with st.sidebar :
    ds = st.date_input("ì¡°íšŒ ì‹œì‘ì¼ ì„ íƒ", pd.to_datetime("2005-06-09"))
    de = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼ ì„ íƒ", pd.to_datetime("2017-01-13"))

# date ì»¬ëŸ¼ ê°’ì„ datetime ê°’ìœ¼ë¡œ ë³€í™˜
df["date"] = pd.to_datetime(df["date"])
# date_inputì— ë”°ë¥¸ ê²°ê³¼ DataFrame ìƒˆë¡œ ìƒì„±
date_df = df[(df["date"] >= pd.Timestamp(ds)) & (df["date"] <= pd.Timestamp(de))]

# graph 1
graph1 = plt.figure(figsize = (10, 7))
plt.rc("font", family = "NanumGothic", size = 13)
plt.rcParams["axes.unicode_minus"] = False
st.subheader("ğŸ“ˆì˜ì‚¬ë¡ ì–´ì¡°ì™€ ê¸°ì¤€ê¸ˆë¦¬ì˜ ë³€í™” ì¶”ì´")
ax1 = date_df.doc_tone.plot(label = "Doc tone")
ax2 = date_df.baserate.plot(label = "Base Rate", secondary_y = True)
# ë²”ë¡€ í‘œì‹œ
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines + lines2, labels + labels2, loc = "upper right")
# yì¶• limit
ax1.set_ylim(-1, 0)
st.pyplot(graph1)

# ì‚¬ì´ë“œ ë°” ë‚ ì§œ ì„ íƒì— ë§ì¶° DataFrame ì¶œë ¥
st.dataframe(date_df, use_container_width = True)

# graph 2
# ì‚°ì ë„, ì¶”ì„¸ì„ 
graph2 = plt.figure()
st.subheader("ğŸ“‰ì˜ì‚¬ë¡ ì–´ì¡°ì— ë”°ë¥¸ ê¸°ì¤€ê¸ˆë¦¬ ë¶„í¬ë„")
sns.regplot(x = "doc_tone", y = "baserate", data = date_df)
plt.xlabel("Doc tone")
plt.ylabel("Base Rate")
st.pyplot(graph2)

# ì„ íƒ ë‚ ì§œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
def get_news_item(url) :
    res = req.get(url)
    soup = bs(res.text, "html.parser")
    date = soup.select_one("span.media_end_head_info_datestamp_time")["data-date-time"]
    title = soup.select_one("h2#title_area").text
    media = soup.select_one("a.media_end_head_top_logo > img")["title"]
    content = soup.select_one("div.newsct_article").text.replace("\n", "")
    return (date, title, media, content)

def get_news(ds, de) :
    page = 1
    result = []
    search = "ê¸ˆë¦¬"
    while True :
        if page == 11 :
            break
        start = (page - 1) * 10 + 1
        url = f"https://s.search.naver.com/p/newssearch/search.naver?de={de}&ds={ds}&eid=&field=0&force_original=&is_dts=0&is_sug_officeid=0&mynews=0&news_office_checked=&nlu_query=&nqx_theme=%7B%22theme%22%3A%7B%22main%22%3A%7B%22name%22%3A%22finance%22%7D%7D%7D&nso=%26nso%3Dso%3Add%2Cp%3Afrom{ds.replace('.', '')}to{de.replace('.', '')}%2Ca%3Aall&nx_and_query=&nx_search_hlquery=&nx_search_query=&nx_sub_query=&office_category=0&office_section_code=0&office_type=0&pd=3&photo=0&query={search}&query_original=&service_area=0&sort=0&spq=0&start={start}&where=news_tab_api&nso=so:dd,p:from{ds.replace('.', '')}to{de.replace('.', '')},a:all"
        res = req.get(url)
        doc = eval(res.text.replace("\n", ""))
        for lst in doc["contents"] :
            soup = bs(lst, "html.parser")
            a_tags = soup.select("div.info_group > a")
            if len(a_tags) == 2 :
                try :
                    result.append(get_news_item(a_tags[-1]["href"]))
                except Exception as e :
                    print("ì˜¤ë¥˜ : ", e)
        page += 1
    return pd.DataFrame(columns = ["date", "title", "media", "content"], data = result)
st.title("ë„¤ì´ë²„ ë‰´ìŠ¤")
st.write(f"{ds.strftime('%Y-%m-%d')}ë¶€í„° {de.strftime('%Y-%m-%d')}ê¹Œì§€ì˜ [ê¸ˆë¦¬] ë„¤ì´ë²„ ë‰´ìŠ¤ 'ê´€ë ¨ë„ìˆœ' ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.")

if ds and de :
    df_news = get_news(ds.strftime("%Y%m%d"), de.strftime("%Y%m%d"))
    st.dataframe(df_news)